{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def split_parquet_files(data_dir='data', random_seed=42):\n",
    "    \"\"\"\n",
    "    将指定目录下的所有parquet文件随机均分为两份，并保存为新文件\n",
    "    \n",
    "    参数:\n",
    "        data_dir (str): 数据根目录，默认'data'\n",
    "        random_seed (int): 随机种子，确保结果可重复，默认42\n",
    "    \"\"\"\n",
    "    data_path = Path(data_dir)\n",
    "    parquet_files = list(data_path.rglob('*.parquet'))\n",
    "    \n",
    "    for file_path in parquet_files:\n",
    "        # 读取数据\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        # 随机均分数据\n",
    "        split1 = df.sample(frac=0.5, random_state=random_seed)\n",
    "        split2 = df.drop(split1.index)\n",
    "        \n",
    "        # 构建新文件路径\n",
    "        parent_dir = file_path.parent\n",
    "        stem = file_path.stem\n",
    "        inf_path = parent_dir / f\"{stem}_inference.parquet\"\n",
    "        eval_path = parent_dir / f\"{stem}_evaluation.parquet\"\n",
    "        \n",
    "        # 保存分割后的数据\n",
    "        split1.to_parquet(inf_path, index=False)\n",
    "        split2.to_parquet(eval_path, index=False)\n",
    "        print(f\"处理完成：{file_path} -> {inf_path}, {eval_path}\")\n",
    "        print(f\"划分数量：Total-{len(df)} infer-{len(split1)} eval-{len(split2)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    split_parquet_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = pd.read_parquet(\"data/cceval/java/test.parquet\")\n",
    "print(len(df_temp))\n",
    "print(df_temp.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp1 = pd.read_parquet(\"data/cceval/java/test_inference.parquet\")\n",
    "print(len(df_temp1))\n",
    "print(df_temp1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp2 = pd.read_parquet(\"data/cceval/java/test_evaluation.parquet\")\n",
    "print(len(df_temp2))\n",
    "print(df_temp2.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "\n",
    "class CodeBlock(object):\n",
    "    def __init__(self, file_path, description, code_content, language, _type):\n",
    "        \"\"\"\n",
    "        Represents a block of code.\n",
    "        :param file_path: The path to the code file.\n",
    "        :param description: The description of the code block.\n",
    "        :param code_content: The content of the code block.\n",
    "        \"\"\"\n",
    "        self.file_path = file_path\n",
    "        self.code_content = code_content\n",
    "        self.description = description\n",
    "        self.language = language\n",
    "        self._type = _type\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.language == \"python\":\n",
    "            comment_label = \"#\"\n",
    "        else:\n",
    "            comment_label = \"//\"\n",
    "        crossfile_context = \"\\n\".join([f\"{comment_label} {cl}\" for cl in  self.description.strip().split('\\n') if cl]) + \"\\n\"\n",
    "        crossfile_context += \"\\n\".join([f\"{comment_label} {cl}\" for cl in  self.code_content.split('\\n') if cl])\n",
    "        return crossfile_context.strip()\n",
    "\n",
    "class Example(object):\n",
    "    def __init__(self, task_id, file_path, left_context, right_context, related_files, target_code, language):\n",
    "        \"\"\"\n",
    "        Represents an example used for constructing a dataset.\n",
    "        :param task_id: Task ID.\n",
    "        :param file_path: File path.\n",
    "        :param left_context: The context to the left of the target code.\n",
    "        :param right_context: The context to the right of the target code.\n",
    "        :param related_files: A list of related files, each containing a path and text.\n",
    "        :param target_code: The target code snippet.\n",
    "        \"\"\"\n",
    "        self.task_id = task_id\n",
    "        self.file_path = file_path\n",
    "        self.left_context = left_context\n",
    "        self.right_context = right_context\n",
    "        self.related_files = related_files\n",
    "        self.target_code = target_code\n",
    "        self.language = language\n",
    "\n",
    "    def __str__(self):\n",
    "        return (f\"[Example]:\\n\"\n",
    "                f\"[Task ID]:\\n{self.task_id}\\n\"\n",
    "                f\"[Path]:\\n{self.file_path}\\n\"\n",
    "                f\"[Left Context]:\\n{self.left_context}\\n\"\n",
    "                f\"[Target Code]:\\n{self.target_code}\\n\"\n",
    "                f\"[Right Context]:\\n{self.right_context}\\n\"\n",
    "                f\"[Related Files]:\\n{len(self.related_files)} files\\n\"\n",
    "        )\n",
    "    \n",
    "# def load_test_dataset(args, datasetname, language):\n",
    "#     \"\"\"\n",
    "#     Loads a dataset.\n",
    "#     :param args: Parameters containing various configurations.\n",
    "#     :param datasetname: The name of the dataset to load.\n",
    "#     :param language: The language of the data to load.\n",
    "#     :return: The loaded dataset.\n",
    "#     \"\"\"\n",
    "#     if datasetname == 'repoeval' and language != 'func_level':\n",
    "#         data_frame1 = pd.read_parquet(f\"data/{datasetname}/{language}/test_0.parquet\")\n",
    "#         data_frame2 = pd.read_parquet(f\"data/{datasetname}/{language}/test_1.parquet\")\n",
    "#         data_frame = pd.concat([data_frame1, data_frame2], ignore_index=True)\n",
    "#     else:\n",
    "#         data_frame = pd.read_parquet(f\"data/{datasetname}/{language}/test.parquet\")\n",
    "\n",
    "#     # data_frame = data_frame.loc[data_frame['task_id'] == 'project_cc_python/210']\n",
    "    \n",
    "#     if datasetname == 'repoeval':\n",
    "#         language = 'python'\n",
    "\n",
    "#     if args.debug:\n",
    "#         data_frame = data_frame.sample(100)\n",
    "#     dataset = []\n",
    "#     for item in data_frame[[\"task_id\", \"path\", \"left_context\", \"right_context\", \"crossfile_context\", \"groundtruth\"]].values:\n",
    "#         cross_files = item[4] if len(item[4]) > 0 else [{'path': \"\", \"text\": \"Don't need cross file context for completion\"}]\n",
    "#         cross_files = [CodeBlock(x[\"path\"], f\"file path: {x['path']}\\nlines: {0}-{len(x['text'].splitlines())}\", x[\"text\"], language, '') for x in cross_files]\n",
    "#         dataset.append(Example(item[0], item[1], item[2], item[3], cross_files, item[5], language))\n",
    "    \n",
    "#     return dataset\n",
    "\n",
    "def load_test_dataset(args, datasetname, language):\n",
    "    \"\"\"\n",
    "    Loads a dataset.\n",
    "    :param args: Parameters containing various configurations.\n",
    "    :param datasetname: The name of the dataset to load.\n",
    "    :param language: The language of the data to load.\n",
    "    :return: The loaded dataset.\n",
    "    \"\"\"\n",
    "    if datasetname == 'repoeval' and language != 'func_level':\n",
    "        data_frame1 = pd.read_parquet(f\"data/{datasetname}/{language}/test_0_inference.parquet\")\n",
    "        data_frame2 = pd.read_parquet(f\"data/{datasetname}/{language}/test_1_inference.parquet\")\n",
    "        data_frame = pd.concat([data_frame1, data_frame2], ignore_index=True)\n",
    "    else:\n",
    "        data_frame = pd.read_parquet(f\"data/{datasetname}/{language}/test_inference.parquet\")\n",
    "\n",
    "    # data_frame = data_frame.loc[data_frame['task_id'] == 'project_cc_python/210']\n",
    "    \n",
    "    if datasetname == 'repoeval':\n",
    "        language = 'python'\n",
    "\n",
    "    if args.debug:\n",
    "        data_frame = data_frame.sample(100)\n",
    "    dataset = []\n",
    "    for item in data_frame[[\"task_id\", \"path\", \"left_context\", \"right_context\", \"crossfile_context\", \"groundtruth\"]].values:\n",
    "        cross_files = item[4] if len(item[4]) > 0 else [{'path': \"\", \"text\": \"Don't need cross file context for completion\"}]\n",
    "        cross_files = [CodeBlock(x[\"path\"], f\"file path: {x['path']}\\nlines: {0}-{len(x['text'].splitlines())}\", x[\"text\"], language, '') for x in cross_files]\n",
    "        dataset.append(Example(item[0], item[1], item[2], item[3], cross_files, item[5], language))\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def load_train_and_valid_dataset():\n",
    "    \"\"\"\n",
    "    Loads the training dataset.\n",
    "    :return: The training dataset.\n",
    "    \"\"\"\n",
    "    training_datasets = []\n",
    "    validation_datasets = []\n",
    "    for language in [\"python\", \"java\"]:\n",
    "        data_frame = pd.read_parquet(f\"data/github_repos/{language}/train.parquet\")\n",
    "        all_data = []\n",
    "        temp_data = []\n",
    "        for x in data_frame[[\"path\", \"content\", \"first\"]].values:\n",
    "            if x[-1]:  # At the start of a new file\n",
    "                if len(temp_data) > 1:\n",
    "                    all_data.append((temp_data,language))\n",
    "                temp_data = []\n",
    "            temp_data.append([x[0], x[1]])\n",
    "        training_datasets.extend(all_data[:2000])\n",
    "        validation_datasets.extend(all_data[2000:2200])\n",
    "    random.shuffle(training_datasets)\n",
    "    random.shuffle(validation_datasets)\n",
    "\n",
    "    return training_datasets, validation_datasets\n",
    "\n",
    "def construct_dataset(raw_data, num_samples):\n",
    "    \"\"\"\n",
    "    Builds a dataset.\n",
    "    :param raw_data: Raw data.\n",
    "    :param num_samples: The number of samples to generate.\n",
    "    :return: The list of constructed samples.\n",
    "    \"\"\"\n",
    "    examples = []\n",
    "    data_index = 0\n",
    "    while len(examples) < num_samples:\n",
    "        example,language = raw_data[data_index % len(raw_data)]\n",
    "        data_index += 1\n",
    "        selected_file = random.choice(example[1:])\n",
    "        related_files = [CodeBlock(x[0], f\"file path: {x[0]}\\nlines: {0}-{len(x[1].splitlines())}\", x[1], language, '') for x in example if x[0] != selected_file[0]]\n",
    "        path = selected_file[0]\n",
    "        selected_file_content = selected_file[1].split(\" \")\n",
    "        try_count = 0\n",
    "\n",
    "        while try_count < 10:\n",
    "            end_line_number = int(len(selected_file_content) * random.uniform(0.2, 0.8))\n",
    "            left_context = \" \".join(selected_file_content[:end_line_number])\n",
    "            target_length = random.randint(32, 64)\n",
    "            target = \" \".join(selected_file_content[end_line_number:end_line_number + target_length])\n",
    "            right_context = \" \".join(selected_file_content[end_line_number + target_length:])\n",
    "            if len(left_context.split()) > 80 and len(target.split()) > 8:\n",
    "                examples.append(\n",
    "                    Example(len(examples), path, left_context, right_context, related_files, target,language)\n",
    "                )\n",
    "                break\n",
    "            # if language == 'python':\n",
    "            #     if len(left_context.split()) > 64 and len(target.split()) > 5:\n",
    "            #         examples.append(\n",
    "            #             Example(len(examples), path, left_context, right_context, related_files, target,language)\n",
    "            #         )\n",
    "            #         break\n",
    "            # elif language == 'java':\n",
    "            #     if len(left_context.split()) > 80 and len(target.split()) > 8:\n",
    "            #         examples.append(\n",
    "            #             Example(len(examples), path, left_context, right_context, related_files, target,language)\n",
    "            #         )\n",
    "            #         break\n",
    "            try_count += 1\n",
    "    \n",
    "    return examples\n",
    "\n",
    "\n",
    "def save_train_for_data_synthesis(examples, file_path=\"data/github_repos/data_for_synthesis/train.parquet\"):\n",
    "    \"\"\"\n",
    "    Saves a list of Example objects to a Parquet file.\n",
    "    :param examples: List of Example objects.\n",
    "    :param file_path: Path to save the Parquet file.\n",
    "    \"\"\"\n",
    "    # Convert Example objects to a dictionary\n",
    "    data = {\n",
    "        'task_id': [example.task_id for example in examples],\n",
    "        'file_path': [example.file_path for example in examples],\n",
    "        'left_context': [example.left_context for example in examples],\n",
    "        'right_context': [example.right_context for example in examples],\n",
    "        'target_code': [example.target_code for example in examples],\n",
    "        'language': [example.language for example in examples],\n",
    "        'related_files': [\n",
    "            [\n",
    "                {\n",
    "                    'file_path': cb.file_path,\n",
    "                    'description': cb.description,\n",
    "                    'code_content': cb.code_content,\n",
    "                    'language': cb.language,\n",
    "                    '_type': cb._type\n",
    "                }\n",
    "                for cb in example.related_files\n",
    "            ]\n",
    "            for example in examples\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Create a DataFrame from the dictionary\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Save the DataFrame to a Parquet file\n",
    "    df.to_parquet(file_path)\n",
    "\n",
    "def load_and_construct_train_for_data_synthesis():\n",
    "    \"\"\"\n",
    "    Loads the training dataset for inference.\n",
    "    :return: The training dataset without valid dataset.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(f\"data/github_repos/data_for_synthesis\"):\n",
    "        # load training dataset and construct\n",
    "        training_datasets = []\n",
    "        for language in [\"python\", \"java\"]:\n",
    "            data_frame = pd.read_parquet(f\"data/github_repos/{language}/train.parquet\")\n",
    "            all_data = []\n",
    "            temp_data = []\n",
    "            for x in data_frame[[\"path\", \"content\", \"first\"]].values:\n",
    "                if x[-1]:  # At the start of a new file\n",
    "                    if len(temp_data) > 1:\n",
    "                        all_data.append((temp_data, language))\n",
    "                    temp_data = []\n",
    "                temp_data.append([x[0], x[1]])\n",
    "            training_datasets.extend(all_data)\n",
    "        training_datasets_examples = construct_dataset(training_datasets, len(training_datasets))\n",
    "        # makedirs and save dataset\n",
    "        os.makedirs(f\"data/github_repos/data_for_synthesis\", exist_ok=True)\n",
    "        save_train_for_data_synthesis(training_datasets_examples)\n",
    "    else:\n",
    "        # Read the Parquet file into a DataFrame\n",
    "        df = pd.read_parquet(\"data/github_repos/data_for_synthesis/train.parquet\")\n",
    "        \n",
    "        # Convert the DataFrame back to a list of Example objects\n",
    "        training_datasets_examples = []\n",
    "        for _, row in df.iterrows():\n",
    "            related_files = [\n",
    "                CodeBlock(\n",
    "                    file_path=cb['file_path'],\n",
    "                    description=cb['description'],\n",
    "                    code_content=cb['code_content'],\n",
    "                    language=cb['language'],\n",
    "                    _type=cb['_type']\n",
    "                )\n",
    "                for cb in row['related_files']\n",
    "            ]\n",
    "            example = Example(\n",
    "                task_id=row['task_id'],\n",
    "                file_path=row['file_path'],\n",
    "                left_context=row['left_context'],\n",
    "                right_context=row['right_context'],\n",
    "                related_files=related_files,\n",
    "                target_code=row['target_code'],\n",
    "                language=row['language']\n",
    "            )\n",
    "            training_datasets_examples.append(example)\n",
    "    return training_datasets_examples\n",
    "\n",
    "training_datasets_examples = load_and_construct_train_for_data_synthesis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"data/github_repos/data_for_synthesis/train.parquet\")\n",
    "\n",
    "# 随机均分数据\n",
    "split1 = df.sample(frac=0.5, random_state=42)\n",
    "split2 = df.drop(split1.index)\n",
    "\n",
    "# 构建新文件路径\n",
    "\n",
    "inf_path = f\"data/github_repos/data_for_synthesis/train_inference.parquet\"\n",
    "eval_path = f\"data/github_repos/data_for_synthesis/train_evaluation.parquet\"\n",
    "\n",
    "# 保存分割后的数据\n",
    "split1.to_parquet(inf_path, index=False)\n",
    "split2.to_parquet(eval_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "读取 data\\repoeval\\api_level\\test_1.parquet 失败: [WinError 32] Failed to open local file 'data/repoeval/api_level/test_1.parquet'. Detail: [Windows error 32] 另一个程序正在使用此文件，进程无法访问。\n",
      "\n",
      "文件: data\\cceval\\java\\test.parquet \t 行数: 2139\n",
      "文件: data\\cceval\\java\\test_evaluation.parquet \t 行数: 1069\n",
      "文件: data\\cceval\\java\\test_inference.parquet \t 行数: 1070\n",
      "文件: data\\cceval\\python\\test.parquet \t 行数: 2665\n",
      "文件: data\\cceval\\python\\test_evaluation.parquet \t 行数: 1333\n",
      "文件: data\\cceval\\python\\test_inference.parquet \t 行数: 1332\n",
      "文件: data\\codereval\\java\\test.parquet \t 行数: 230\n",
      "文件: data\\codereval\\java\\test_evaluation.parquet \t 行数: 115\n",
      "文件: data\\codereval\\java\\test_inference.parquet \t 行数: 115\n",
      "文件: data\\codereval\\python\\test.parquet \t 行数: 230\n",
      "文件: data\\codereval\\python\\test_evaluation.parquet \t 行数: 115\n",
      "文件: data\\codereval\\python\\test_inference.parquet \t 行数: 115\n",
      "文件: data\\github_repos\\data_for_synthesis\\train_evaluation.parquet \t 行数: 3368\n",
      "文件: data\\github_repos\\data_for_synthesis\\train_inference.parquet \t 行数: 3368\n",
      "文件: data\\github_repos\\java\\train.parquet \t 行数: 79989\n",
      "文件: data\\github_repos\\java\\train_evaluation.parquet \t 行数: 39995\n",
      "文件: data\\github_repos\\java\\train_inference.parquet \t 行数: 39994\n",
      "文件: data\\github_repos\\python\\train.parquet \t 行数: 38497\n",
      "文件: data\\github_repos\\python\\train_evaluation.parquet \t 行数: 19249\n",
      "文件: data\\github_repos\\python\\train_inference.parquet \t 行数: 19248\n",
      "文件: data\\repoeval\\api_level\\test_0.parquet \t 行数: 800\n",
      "文件: data\\repoeval\\api_level\\test_0_evaluation.parquet \t 行数: 400\n",
      "文件: data\\repoeval\\api_level\\test_0_inference.parquet \t 行数: 400\n",
      "文件: data\\repoeval\\api_level\\test_1.parquet \t 行数: None\n",
      "文件: data\\repoeval\\api_level\\test_1_inference.parquet \t 行数: 400\n",
      "文件: data\\repoeval\\func_level\\test.parquet \t 行数: 455\n",
      "文件: data\\repoeval\\func_level\\test_evaluation.parquet \t 行数: 227\n",
      "文件: data\\repoeval\\func_level\\test_inference.parquet \t 行数: 228\n",
      "文件: data\\repoeval\\line_level\\test_0.parquet \t 行数: 800\n",
      "文件: data\\repoeval\\line_level\\test_0_evaluation.parquet \t 行数: 400\n",
      "文件: data\\repoeval\\line_level\\test_0_inference.parquet \t 行数: 400\n",
      "文件: data\\repoeval\\line_level\\test_1.parquet \t 行数: 800\n",
      "文件: data\\repoeval\\line_level\\test_1_evaluation.parquet \t 行数: 400\n",
      "文件: data\\repoeval\\line_level\\test_1_inference.parquet \t 行数: 400\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "def get_parquet_row_counts(folder_path):\n",
    "    folder = Path(folder_path)\n",
    "    parquet_files = folder.glob('**/*.parquet')  # 递归获取所有parquet文件\n",
    "    \n",
    "    results = []\n",
    "    for file in parquet_files:\n",
    "        try:\n",
    "            # 使用pyarrow直接读取元数据（不加载实际数据）\n",
    "            parquet_file = pq.ParquetFile(file)\n",
    "            row_count = parquet_file.metadata.num_rows\n",
    "            results.append((str(file), row_count))\n",
    "        except Exception as e:\n",
    "            print(f\"读取 {file} 失败: {str(e)}\")\n",
    "            results.append((str(file), None))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 使用示例\n",
    "folder_path = \"./data\"\n",
    "results = get_parquet_row_counts(folder_path)\n",
    "\n",
    "# 打印结果\n",
    "for file, count in results:\n",
    "    print(f\"文件: {file} \\t 行数: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "d2l",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
